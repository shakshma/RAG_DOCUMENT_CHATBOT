{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-544MNH01rM"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf langchain langchain-community sentence-transformers faiss-cpu transformers torch -q\n",
        "# -q for quiet installation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# notebooks/2_llm_testing_and_prompt_engineering.ipynb\n",
        "\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# --- Configuration ---\n",
        "# Make sure you have enough VRAM/RAM for this model!\n",
        "LLM_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# Your Hugging Face access token\n",
        "# IMPORTANT: Replace \"YOUR_HF_TOKEN_HERE\" with your actual token.\n",
        "# For Google Colab, it's generally recommended to use Colab Secrets (the key icon on the left)\n",
        "# and set the secret name to HF_TOKEN. If you do that, you don't need this line.\n",
        "# However, for direct testing or if secrets aren't working, passing it explicitly can help.\n",
        "HF_ACCESS_TOKEN = \"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\" # <--- REPLACE WITH YOUR ACTUAL TOKEN\n",
        "\n",
        "# --- Load LLM (same logic as in src/generator.py) ---\n",
        "print(f\"Loading tokenizer: {LLM_MODEL_ID}\")\n",
        "# Pass the token explicitly to from_pretrained\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_ID, token=HF_ACCESS_TOKEN)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"Set pad_token to eos_token for the tokenizer.\")\n",
        "\n",
        "print(f\"Loading model: {LLM_MODEL_ID}. This may take some time and requires ~14GB VRAM/RAM.\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\" and torch.cuda.get_device_capability()[0] >= 8: # Check for Ampere or newer\n",
        "    torch_dtype = torch.bfloat16\n",
        "    print(f\"Using bfloat16 on {device}.\")\n",
        "else:\n",
        "    torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
        "    print(f\"Using {torch_dtype} on {device}.\")\n",
        "\n",
        "# Pass the token explicitly to from_pretrained\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    LLM_MODEL_ID,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device_map=\"auto\",\n",
        "    token=HF_ACCESS_TOKEN # <--- Pass the token here as well\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(\"Setting up text-generation pipeline...\")\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.1,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    return_full_text=False,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "print(\"LLM and pipeline loaded.\")\n",
        "\n",
        "# --- Prompt Template Experimentation ---\n",
        "\n",
        "# Version 1: Simple Prompt\n",
        "print(\"\\n--- Testing Simple Prompt ---\")\n",
        "simple_template = \"\"\"\n",
        "[INST] Question: {question}\n",
        "Context: {context} [/INST]\n",
        "\"\"\"\n",
        "simple_prompt = PromptTemplate.from_template(simple_template)\n",
        "\n",
        "simple_chain = (\n",
        "    simple_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "context_for_test = \"\"\"\n",
        "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France.\n",
        "It was constructed from 1887 to 1889 as the entrance to the 1889 World's Fair.\n",
        "\"\"\"\n",
        "question_for_test = \"When was the Eiffel Tower built and where is it located?\"\n",
        "\n",
        "response_simple = simple_chain.invoke({\"question\": question_for_test, \"context\": context_for_test})\n",
        "print(f\"Question: {question_for_test}\")\n",
        "print(f\"Response: {response_simple.strip()}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Version 2: More Instructive Prompt (Similar to app.py/generator.py)\n",
        "print(\"\\n--- Testing Instructive Prompt ---\")\n",
        "instructive_template = \"\"\"\n",
        "[INST] You are an intelligent AI assistant dedicated to providing accurate and concise answers based solely on the provided context.\n",
        "If the question cannot be answered from the provided context, please state \"I cannot answer this question based on the provided information.\" Do not use outside knowledge.\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Context: {context} [/INST]\n",
        "\"\"\"\n",
        "instructive_prompt = PromptTemplate.from_template(instructive_template)\n",
        "\n",
        "instructive_chain = (\n",
        "    instructive_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "question_unanswerable = \"What is the airspeed velocity of an unladen swallow?\"\n",
        "response_unanswerable = instructive_chain.invoke({\"question\": question_unanswerable, \"context\": context_for_test})\n",
        "print(f\"Question: {question_unanswerable}\")\n",
        "print(f\"Response: {response_unanswerable.strip()}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Another test with a different context\n",
        "print(\"\\n--- Testing another scenario with Instructive Prompt ---\")\n",
        "another_context = \"\"\"\n",
        "Quantum computing is a type of computing that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data.\n",
        "Unlike classical computers that use bits which can be either 0 or 1, quantum computers use qubits that can represent 0, 1, or both simultaneously.\n",
        "\"\"\"\n",
        "another_question = \"Explain qubits in simple terms.\"\n",
        "response_another = instructive_chain.invoke({\"question\": another_question, \"context\": another_context})\n",
        "print(f\"Question: {another_question}\")\n",
        "print(f\"Response: {response_another.strip()}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(\"\\nPrompt engineering experimentation complete. You can modify the templates and contexts above to further refine your prompt.\")"
      ],
      "metadata": {
        "id": "_nM7kMBr1F3s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}