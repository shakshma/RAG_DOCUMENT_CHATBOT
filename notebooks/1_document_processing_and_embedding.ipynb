{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Qp-e1uJBW5l"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf langchain sentence-transformers faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cxKxQxM0CSdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define your base project directory in Google Drive\n",
        "# I recommend creating a dedicated folder in MyDrive, e.g., 'my_rag_chatbot_project'\n",
        "base_drive_path = \"/content/drive/MyDrive/my_rag_chatbot_project\"\n",
        "\n",
        "# Create the necessary subdirectories\n",
        "os.makedirs(os.path.join(base_drive_path, \"data\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(base_drive_path, \"chunks\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(base_drive_path, \"vectordb\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(base_drive_path, \"notebooks\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(base_drive_path, \"src\"), exist_ok=True)\n",
        "\n",
        "print(f\"Project directories created in: {base_drive_path}\")"
      ],
      "metadata": {
        "id": "Ye1crWALCmhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf langchain langchain-community sentence-transformers faiss-cpu transformers torch -q\n",
        "# -q for quiet installation"
      ],
      "metadata": {
        "id": "E3MGIAqCDRQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# notebooks/1_document_processing_and_embedding.ipynb (Colab Version)\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "# Make sure you've mounted drive and created directories first as per steps 2 & 3 above.\n",
        "# from google.colab import files # If you plan to upload file via code\n",
        "\n",
        "# Define your base project directory in Google Drive\n",
        "base_drive_path = \"/content/drive/MyDrive/my_rag_chatbot_project\"\n",
        "\n",
        "# --- Step 2: Load the document ---\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Define the path to your PDF document in Google Drive\n",
        "pdf_path = os.path.join(base_drive_path, \"data\", \"chatdata.pdf\")\n",
        "\n",
        "if not os.path.exists(pdf_path):\n",
        "    print(f\"Error: PDF not found at {pdf_path}. Please upload it to your Google Drive.\")\n",
        "else:\n",
        "    print(f\"Loading document from: {pdf_path}\")\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    documents = loader.load()\n",
        "    print(f\"Loaded {len(documents)} pages.\")\n",
        "\n",
        "    full_document_content = \"\\n\".join([doc.page_content for doc in documents])\n",
        "    print(f\"Content length before chunking: {len(full_document_content)} characters.\")\n",
        "\n",
        "    # --- Step 3: Clean and format the text (if needed) ---\n",
        "    cleaned_content = full_document_content # Placeholder for actual cleaning if required later\n",
        "\n",
        "    # --- Step 4: Chunk the documents into 100-300 word segments using sentence-aware splitting ---\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1200,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False,\n",
        "    )\n",
        "\n",
        "    chunks = text_splitter.create_documents([cleaned_content])\n",
        "    print(f\"Generated {len(chunks)} chunks.\")\n",
        "\n",
        "    # Optional: Save chunks to a file in Google Drive for review\n",
        "    import json\n",
        "    chunk_data = [{\"page_content\": chunk.page_content, \"metadata\": chunk.metadata} for chunk in chunks]\n",
        "    chunks_save_path = os.path.join(base_drive_path, \"chunks\", \"processed_chunks.json\")\n",
        "    with open(chunks_save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(chunk_data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Chunks saved to {chunks_save_path}\")\n",
        "\n",
        "    # --- Step 5: Generate embeddings using a pre-trained model ---\n",
        "    from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "\n",
        "    embedding_model_name = \"all-MiniLM-L6-v2\"\n",
        "    print(f\"Loading embedding model: {embedding_model_name}\")\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=embedding_model_name)\n",
        "    print(\"Embedding model loaded.\")\n",
        "\n",
        "    # --- Step 6: Store the embeddings in a vector database (FAISS) ---\n",
        "    from langchain_community.vectorstores import FAISS\n",
        "\n",
        "    print(\"Creating FAISS vector database...\")\n",
        "    vector_db = FAISS.from_documents(chunks, embeddings)\n",
        "    print(\"FAISS vector database created.\")\n",
        "\n",
        "    # --- Step 7: Save the vector database to Google Drive ---\n",
        "    faiss_save_path = os.path.join(base_drive_path, \"vectordb\", \"faiss_index\")\n",
        "    vector_db.save_local(faiss_save_path)\n",
        "    print(f\"FAISS index saved to {faiss_save_path}\")\n",
        "\n",
        "    print(\"Document processing and embedding complete!\")"
      ],
      "metadata": {
        "id": "t92nGKe_DzBj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}